{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqMRYcEfCBjX",
        "outputId": "0e065a8a-4bb5-4b18-d515-32125fc37cb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-06-03T18:35:45.57732Z",
          "iopub.execute_input": "2022-06-03T18:35:45.578165Z",
          "iopub.status.idle": "2022-06-03T18:35:46.161519Z",
          "shell.execute_reply.started": "2022-06-03T18:35:45.578054Z",
          "shell.execute_reply": "2022-06-03T18:35:46.160726Z"
        },
        "trusted": true,
        "id": "-HdOMrj0Bxnn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data and Show information"
      ],
      "metadata": {
        "id": "37B-dxKfRL6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/Undergrad/Semester-7/DWDM_Preprocessing-Assignment/Data\"\n",
        "df = pd.read_csv(os.path.join(DATA_PATH, \"taxi_trip_data.csv\"))\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "-LM_3j5FB_Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show first 5 rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:35:46.162902Z",
          "iopub.execute_input": "2022-06-03T18:35:46.163105Z",
          "iopub.status.idle": "2022-06-03T18:36:29.995733Z",
          "shell.execute_reply.started": "2022-06-03T18:35:46.16308Z",
          "shell.execute_reply": "2022-06-03T18:36:29.994898Z"
        },
        "trusted": true,
        "id": "bMmvA-f7Bxnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "The goal of this notebook is to clean and transform the data available for the purpose of later utilizing it in ML algorithms, or for data warehousing purposed.\n",
        "\n",
        "The data cleaning process can begin to clear out outliers, missing values and other noise which might affect the results of the algorithm. \n",
        "\n"
      ],
      "metadata": {
        "id": "YTj-JBfWBxnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "\n",
        "Rides following similar paths in the past will likely take similar routes, and rides during the same hours of the day will also likely take roughly the same amount of time. This gives us a sort of rolling average for distance and time to make the calculation easier, what it doesn't give us, is how much of that distance is sitting in traffic, below 12mph, or driving at normal speeds above 12mph, nor does it account for sitting at red lights.\n",
        "\n",
        "These values are hard to account for. While patterns can be detected when analysing the data through graphs and other visuals, it doesn't make for a very mathematical or repeatable prediction. We'll need the model to detect these patterns quickly and repeatably to get the most accurate predictions possible, which means some data, such as start and end times should be broken down into chunks that are easier for a machine to read such as the number of minutes per trip, the month, day, day of the week, and year (separately) .\n",
        "\n"
      ],
      "metadata": {
        "id": "l4IHy026XFw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset: NYC Taxi Trip Data - Google Public Data\n",
        "\n",
        "This data set is a subset of the Google BigQuery public datasets - Nyc yellow taxi cab trips data set containing a random 10,000,000 rows of data. This dataset was extracted and uploaded for the purpose of experimenting with and learning regression models for price prediction. There is also a lot of room for data cleaning, outliers in the data, and plenty of data to work with for more realistic model training, testing, and validation."
      ],
      "metadata": {
        "id": "OS7Med0WXL61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Attributes\n",
        "\n",
        "| column            | type     | nullable | description                                                                                                                                                                                                                                          |\n",
        "| ----------------- | -------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| vendor\\_id        | text     | required | A code indicating the TPEP provider that provided the record. 1= Creative Mobile Technologies, LLC; 2= VeriFone Inc                                                                                                                                  |\n",
        "| pickup\\_datetime  | datetime | nullable | The date and time when the meter was engaged.                                                                                                                                                                                                        |\n",
        "| dropoff\\_datetime | datetime | nullable | The date and time when the meter was disengaged.                                                                                                                                                                                                     |\n",
        "| passenger\\_count  | integer  | nullable | The number of passengers in the vehicle. This is a driver-entered value                                                                                                                                                                              |\n",
        "| trip\\_distance    | numeric  | nullable | The elapsed trip distance in miles reported by the taximeter.                                                                                                                                                                                        |\n",
        "| rate\\_code        | string   | nullable | The final rate code in effect at the end of the trip. 1= Standard rate 2=JFK 3=Newark 4=Nassau or Westchester 5=Negotiated fare 6=Group ride                                                                                                         |\n",
        "| storeandfwd\\_flag | string   | nullable | This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server. Y= store and forward trip N= not a store and forward trip |\n",
        "| payment\\_type     | string   | nullable | A numeric code signifying how the passenger paid for the trip. 1= Credit card 2= Cash 3= No charge 4= Dispute 5= Unknown 6= Voided trip                                                                                                              |\n",
        "| fare\\_amount      | numeric  | nullable | The time-and-distance fare calculated by the meter                                                                                                                                                                                                   |\n",
        "| extra             | numeric  | nullable | Miscellaneous extras and surcharges. Currently, this only includes the \\\\$0.50 and \\\\$1 rush hour and overnight charges.                                                                                                                             |\n",
        "| mta\\_tax          | numeric  | nullable | \\\\$0.50 MTA tax that is automatically triggered based on the metered rate in use                                                                                                                                                                     |\n",
        "| tip\\_amount       | numeric  | nullable | Tip amount – This field is automatically populated for credit card tips. Cash tips are not included                                                                                                                                                  |\n",
        "| tolls\\_amount     | numeric  | nullable | Total amount of all tolls paid in the trip.                                                                                                                                                                                                          |\n",
        "| imp\\_surcharge    | numeric  | nullable | \\\\$0.30 improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015.                                                                                                                                 |\n",
        "| total\\_amount     | numeric  | nullable | The total amount charged to passengers. Does not include cash tips                                                                                                                                                                                   |\n",
        "| pickuplocationid  | string   | nullable | TLC Taxi Zone in which the taximeter was engaged                                                                                                                                                                                                     |\n",
        "| dropofflocationid | string   | nullable | TLC Taxi Zone in which the taximeter was disengaged                        "
      ],
      "metadata": {
        "id": "b1_lQbASXSIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan for Features  \n",
        "\n",
        "Here are the features of the current dataset that will be kept, as well as a few that will need to be created based on other features:  \n",
        "- pickup_timestamp  \n",
        "- dropoff_timestamp  \n",
        "- trip_distance  \n",
        "- fare_amount  \n",
        "- extra  \n",
        "- mta_tax  \n",
        "- imp_surcharge  \n",
        "- total_amount  \n",
        "- pickup_location_id  \n",
        "- dropoff_location_id  "
      ],
      "metadata": {
        "id": "l4ketGWKXWBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis\n",
        "\n",
        "The **correlation matrix** calculates how the change in one value effects a change in the other value, and assigns a value between -1 and 1 to that correlation.  \n",
        "Let's review what those correlation values mean before we move on:  \n"
      ],
      "metadata": {
        "id": "KnpqGbHPEzkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Matrix\n",
        "\n",
        "The correlation matrix for all pairs of attributes is represented in the heatmap below:\n",
        "\n",
        "* **-1:** A very strong negative correlation, when value A moves in one direction, value B moves in the opposite direction.  \n",
        "* **0:**  No correlation between values A and B, when one moves, the other is not effected.  \n",
        "* **1:**  A very strong positive correlation, as you can guess, this is the opposite of the negative correlation above. When value A moves in one direction, value B follows in the same direction.  \n",
        "\n",
        "This value isn't related to the rate of change, only the direction of change. Value A moves up, and value B either stays, moves up, or moves down. "
      ],
      "metadata": {
        "id": "_h-wdpOwXaaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the correlation matrix\n",
        "corr = df.corr()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:36:29.996897Z",
          "iopub.execute_input": "2022-06-03T18:36:29.997122Z",
          "iopub.status.idle": "2022-06-03T18:36:36.515903Z",
          "shell.execute_reply.started": "2022-06-03T18:36:29.997096Z",
          "shell.execute_reply": "2022-06-03T18:36:36.514896Z"
        },
        "trusted": true,
        "id": "2mqfDfxsBxn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drawing the heatmap\n",
        "fig, ax = plt.subplots(figsize=(15,15))\n",
        "ax = sns.heatmap(corr, cmap='YlGnBu', annot=True, linewidths=0.5);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:36:36.518416Z",
          "iopub.execute_input": "2022-06-03T18:36:36.518655Z",
          "iopub.status.idle": "2022-06-03T18:36:37.747701Z",
          "shell.execute_reply.started": "2022-06-03T18:36:36.518625Z",
          "shell.execute_reply": "2022-06-03T18:36:37.746809Z"
        },
        "trusted": true,
        "id": "x8zCBGZPBxn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is already a list of known values that we should keeep. The only remaining values are:  \n",
        "- **vendor_id** - Vendor of data provider. This definitely won't be used for anything for our model here  \n",
        "- **rate_code** - The rate code at the end of the trip. Used likely to track certain charges. Has a correlation with tolls and tips but not much with anything else.  \n",
        "- **sotre_and_fwd_flag** - This is simply a fag that indicates whether a value was stored in vehicle memory before being recorded due to a lack of internet connection. This is useless to us, however, it's currently stored as a string and converting it to a value that can appear in a correlation matrix later might serve useful. While not likely, it could be possible that values are different from those not stored in memory, such as having a higher amount of errors, or some upload process might be altering values in an unexpected way.\n",
        "\n",
        "In the end, only one column is being dropped right off the start and that's **Vendor ID**. "
      ],
      "metadata": {
        "id": "BtecLfUJBxn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('vendor_id', axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:36:37.749039Z",
          "iopub.execute_input": "2022-06-03T18:36:37.749305Z",
          "iopub.status.idle": "2022-06-03T18:36:41.193819Z",
          "shell.execute_reply.started": "2022-06-03T18:36:37.749271Z",
          "shell.execute_reply": "2022-06-03T18:36:41.192781Z"
        },
        "trusted": true,
        "id": "1POpmnu7Bxn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning  \n",
        "\n",
        "The following data cleaning steps are assessed and applied:\n",
        "1. **Remove duplicate rows** - Carefully, as we only want to remove duplicate trips, not duplicates within the values themselves. These values are not required to be unique.\n",
        "2. Check for **missing values**\n",
        "3. Check for **zeros and empty strings**. These values won't be \"missing\" but still aren't valid. Very few columns in this data have valid zeros  \n",
        "4.**Validate formatting** of data, especially dates  \n",
        "5. **Strip and normalize strings** - our data doesn't contain any strings, so we can skip this.  "
      ],
      "metadata": {
        "id": "CPY1o2G2BxoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Duplicates"
      ],
      "metadata": {
        "id": "-2C8aOZIIM0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates - \n",
        "# Rename the dataframe from df to td for temporary data, thus not altering the original dataframe until much later. \n",
        "td = df.drop_duplicates()\n",
        "# less than 1% dropped\n",
        "print(f\"{df.shape[0] - td.shape[0]} duplicate rows dropped. Thats {(df.shape[0] - td.shape[0]) / df.shape[0] * 100}%\")\n",
        "print(f\"{td.shape[0]} rows remain.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:36:41.196184Z",
          "iopub.execute_input": "2022-06-03T18:36:41.196682Z",
          "iopub.status.idle": "2022-06-03T18:37:11.105035Z",
          "shell.execute_reply.started": "2022-06-03T18:36:41.196626Z",
          "shell.execute_reply": "2022-06-03T18:37:11.10394Z"
        },
        "trusted": true,
        "id": "obkg8aypBxoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Missing Values"
      ],
      "metadata": {
        "id": "Z9uZdAmzMEMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "for col in td.columns:\n",
        "    missing = td[col].isna().sum()\n",
        "    print(f\"Missing values in {col}: {missing}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:11.106443Z",
          "iopub.execute_input": "2022-06-03T18:37:11.106768Z",
          "iopub.status.idle": "2022-06-03T18:37:12.531739Z",
          "shell.execute_reply.started": "2022-06-03T18:37:11.106735Z",
          "shell.execute_reply": "2022-06-03T18:37:12.530897Z"
        },
        "trusted": true,
        "id": "AKWhSuuMBxoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Zeros and Empty Strings"
      ],
      "metadata": {
        "id": "N93GxNUhMJ6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for zeros in numeric columns\n",
        "def check_for_zeros(td):\n",
        "    for col in td.columns:\n",
        "        zeros = td[td[col] == 0].shape[0]\n",
        "        print(f\"Zeros in {col}:{zeros}\")\n",
        "        \n",
        "check_for_zeros(td)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:12.53374Z",
          "iopub.execute_input": "2022-06-03T18:37:12.534005Z",
          "iopub.status.idle": "2022-06-03T18:37:16.476467Z",
          "shell.execute_reply.started": "2022-06-03T18:37:12.533976Z",
          "shell.execute_reply": "2022-06-03T18:37:16.475416Z"
        },
        "trusted": true,
        "id": "eu25PJxHBxoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Changes applied so far ...**\n",
        "\n",
        "* *passenger_count*, *trip_distance*, *fare_amount* and *total_amount* --- all contain zeros.\n",
        "\n",
        "* It doesn't appear to be a large amount of the overall data.\n",
        "\n",
        "* Without distance, we can't determine fare amount, even with distance, it's impossible to know which miles were driven above the 12mph threshold, and which were below.\n",
        "\n",
        "* There isn't much of a choice but to drop these. However, **total_amount** can be corrected by simply adding all of the charge column values together, so I'll keep and fix these rows."
      ],
      "metadata": {
        "id": "v_89e5VyBxoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropping rows with 0 values in columns where 0 is not allowed**"
      ],
      "metadata": {
        "id": "QXQiszXlRicQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping rows with 0 values in columns where 0 is not allowed\n",
        "td = td.drop(['passenger_count'], axis=1)\n",
        "td = td[td['trip_distance'] > 0]\n",
        "td = td[td['fare_amount'] > 0]\n",
        "\n",
        "check_for_zeros(td)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:16.477705Z",
          "iopub.execute_input": "2022-06-03T18:37:16.478029Z",
          "iopub.status.idle": "2022-06-03T18:37:22.865542Z",
          "shell.execute_reply.started": "2022-06-03T18:37:16.477994Z",
          "shell.execute_reply": "2022-06-03T18:37:22.86461Z"
        },
        "trusted": true,
        "id": "q2vbPo7LBxoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After dropping rows with zero values in other columns, there remains no zeros in total_amount, so no corrections are necessary here"
      ],
      "metadata": {
        "id": "eARfwpfZBxoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how much of the original data ramains\n",
        "remaining = td.shape[0] / df.shape[0] * 100\n",
        "print(f\"Remaining amount of original dataset: {remaining}%\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:22.869606Z",
          "iopub.execute_input": "2022-06-03T18:37:22.869872Z",
          "iopub.status.idle": "2022-06-03T18:37:22.874779Z",
          "shell.execute_reply.started": "2022-06-03T18:37:22.869826Z",
          "shell.execute_reply": "2022-06-03T18:37:22.873892Z"
        },
        "trusted": true,
        "id": "-GGqSOYTBxoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating Data Formats\n",
        "  \n",
        "Ensure that the dates are all readable date formats and exist in the same format such as mm/dd/yyyy, for example. "
      ],
      "metadata": {
        "id": "TSPB9Ud_BxoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to an actual Python/Pandas datetime object ensures that the data is a valid datetime. \n",
        "# Then, we  move on to exploring the datetimes available.\n",
        "td['pickup_datetime'] = pd.to_datetime(td['pickup_datetime'])\n",
        "td['dropoff_datetime'] = pd.to_datetime(td['dropoff_datetime'])\n",
        "\n",
        "print('Done.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:22.875857Z",
          "iopub.execute_input": "2022-06-03T18:37:22.876263Z",
          "iopub.status.idle": "2022-06-03T18:37:26.771524Z",
          "shell.execute_reply.started": "2022-06-03T18:37:22.876232Z",
          "shell.execute_reply": "2022-06-03T18:37:26.770586Z"
        },
        "trusted": true,
        "id": "wGgN7jn2BxoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** All datetime stamps in the dataset are correctly formatted"
      ],
      "metadata": {
        "id": "YWr_zb2hLiN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datetime columns are now split up into meaninful columns. The only dropoff information we really need to keep is the hour, and even then, only to calculate the length of the trip. "
      ],
      "metadata": {
        "id": "kMM_8BafBxoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td['year'] = pd.to_datetime(td['pickup_datetime']).dt.year\n",
        "td['month'] = pd.to_datetime(td['pickup_datetime']).dt.month\n",
        "td['day'] = pd.to_datetime(td['pickup_datetime']).dt.day\n",
        "td['day_of_week'] = pd.to_datetime(td['pickup_datetime']).dt.dayofweek\n",
        "td['hour_of_day'] = pd.to_datetime(td['pickup_datetime']).dt.hour\n",
        "\n",
        "print('Done.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:26.773009Z",
          "iopub.execute_input": "2022-06-03T18:37:26.773346Z",
          "iopub.status.idle": "2022-06-03T18:37:31.460753Z",
          "shell.execute_reply.started": "2022-06-03T18:37:26.773303Z",
          "shell.execute_reply": "2022-06-03T18:37:31.459896Z"
        },
        "trusted": true,
        "id": "p1JuzGENBxoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Date and Time Data"
      ],
      "metadata": {
        "id": "AV_u_2WvBxoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate Timestamps"
      ],
      "metadata": {
        "id": "GD8xnRNeT5Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the datetime columns to a numpy array for vectorization\n",
        "pickup_array = td['pickup_datetime'].values\n",
        "dropoff_array = td['dropoff_datetime'].values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:31.461967Z",
          "iopub.execute_input": "2022-06-03T18:37:31.462174Z",
          "iopub.status.idle": "2022-06-03T18:37:32.135003Z",
          "shell.execute_reply.started": "2022-06-03T18:37:31.462149Z",
          "shell.execute_reply": "2022-06-03T18:37:32.134241Z"
        },
        "trusted": true,
        "id": "4AmcVRMZBxoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate Trip Durations"
      ],
      "metadata": {
        "id": "lQoWUHFoUL8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the new timedelta, this takes less than a second to complete compared to 15+ minutes with apply()\n",
        "trip_duration = np.subtract(dropoff_array, pickup_array)\n",
        "\n",
        "# Adding the resulting array to the dataframe in the trip_duration column\n",
        "td['trip_duration'] = pd.Series(trip_duration)\n",
        "\n",
        "# Converting the timedelta to number of seconds\n",
        "td['trip_duration'] = td['trip_duration'].dt.total_seconds()\n",
        "\n",
        "# Preview the results\n",
        "td.head()"
      ],
      "metadata": {
        "id": "EPszeUJWUJT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the datetime columns can be dropped entirely. "
      ],
      "metadata": {
        "id": "r-c8FgpeBxoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td.drop(['pickup_datetime', 'dropoff_datetime'], axis=1, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:32.136086Z",
          "iopub.execute_input": "2022-06-03T18:37:32.136293Z",
          "iopub.status.idle": "2022-06-03T18:37:34.033042Z",
          "shell.execute_reply.started": "2022-06-03T18:37:32.136268Z",
          "shell.execute_reply": "2022-06-03T18:37:34.0322Z"
        },
        "trusted": true,
        "id": "v4tgMDdLBxoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the dataset's current state ..."
      ],
      "metadata": {
        "id": "ZC9_3p7vNnDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:34.034229Z",
          "iopub.execute_input": "2022-06-03T18:37:34.034516Z",
          "iopub.status.idle": "2022-06-03T18:37:34.057464Z",
          "shell.execute_reply.started": "2022-06-03T18:37:34.034478Z",
          "shell.execute_reply": "2022-06-03T18:37:34.056615Z"
        },
        "trusted": true,
        "id": "xoU3vBE5BxoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the dates have been broken down properly, a higher level of data clean-up can be performed.\n",
        "\n",
        "* Any trips with a duration of 0 need to be dropped. These trips won't be useful, and are certainly due to a data entry error.  \n",
        "\n",
        "* Investigate what years are available in this dataset, how much of the dataset each year makes up, and begin investigating whether we should keep all years, or only specific years by visualizing trends in fare amounts when compared to trip duration and distance. "
      ],
      "metadata": {
        "id": "ywsiY6JzBxoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td = td[td['trip_duration'] > 0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:34.058671Z",
          "iopub.execute_input": "2022-06-03T18:37:34.059374Z",
          "iopub.status.idle": "2022-06-03T18:37:34.76337Z",
          "shell.execute_reply.started": "2022-06-03T18:37:34.059327Z",
          "shell.execute_reply": "2022-06-03T18:37:34.762809Z"
        },
        "trusted": true,
        "id": "MDYVqa0lBxoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_years = td.year.unique()\n",
        "print(list_of_years)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:34.76474Z",
          "iopub.execute_input": "2022-06-03T18:37:34.764988Z",
          "iopub.status.idle": "2022-06-03T18:37:34.820304Z",
          "shell.execute_reply.started": "2022-06-03T18:37:34.764956Z",
          "shell.execute_reply": "2022-06-03T18:37:34.819338Z"
        },
        "trusted": true,
        "id": "fdF464USBxoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for year in list_of_years:\n",
        "    year_amount = td[td['year'] == year].shape[0]\n",
        "    total_amount = td.shape[0]\n",
        "    \n",
        "    print(f\"{year} makes up {(year_amount / total_amount) * 100}% of the dataset\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:34.821623Z",
          "iopub.execute_input": "2022-06-03T18:37:34.822382Z",
          "iopub.status.idle": "2022-06-03T18:37:35.775449Z",
          "shell.execute_reply.started": "2022-06-03T18:37:34.822335Z",
          "shell.execute_reply": "2022-06-03T18:37:35.774873Z"
        },
        "trusted": true,
        "id": "zH6yOcxkBxoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminate Off-Trend Data\n",
        "\n",
        "It's clear that this dataset is HEAVILY weighted towards 2018. For that reason, dropping anything from before 2018 can help avoid skewing the data towards old trends, while keeping anything newer than 2018 might reveal new trends. \n",
        "\n",
        "If a dataset of such massive size consists of 99% of the same year, it's likely that the trips from newer years are either invalid data upon collection, and incomplete enough to actually show any trends.  \n",
        "\n",
        "All rows but 2018 are, therefore, dropped. "
      ],
      "metadata": {
        "id": "B-yXSZioBxoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td = td[td['year'] == 2018]\n",
        "# Evaluate data stats after dropping\n",
        "td.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:35.776527Z",
          "iopub.execute_input": "2022-06-03T18:37:35.776897Z",
          "iopub.status.idle": "2022-06-03T18:37:42.348593Z",
          "shell.execute_reply.started": "2022-06-03T18:37:35.776861Z",
          "shell.execute_reply": "2022-06-03T18:37:42.347611Z"
        },
        "trusted": true,
        "id": "qsZqnH-VBxoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trip Fare - Sanity Checks\n",
        "\n",
        "The value of *total_amount* should be equal to the sum of the *fare_amount*, *mta_tax*, *tip_amount*, *tolls_amount*, *imp_surcharge* and the *extra*.\n",
        "\n",
        "Calculating total amounts and dropping rows whose values don't \"add up\"..."
      ],
      "metadata": {
        "id": "Eup2MTxaPYbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop Fare Columns with Negative Values"
      ],
      "metadata": {
        "id": "pWKsUJbJY59z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_count = len(td)\n",
        "td = td[td['fare_amount'] >= 0]\n",
        "td = td[td['extra'] >= 0]\n",
        "td = td[td['mta_tax'] >= 0]\n",
        "td = td[td['tip_amount'] >= 0]\n",
        "td = td[td['imp_surcharge'] >= 0]\n",
        "td = td[td['tolls_amount'] >= 0]\n",
        "final_count = len(td)\n",
        "\n",
        "print(f\"Fraction of dataframe retained: {final_count / init_count * 100}%\")"
      ],
      "metadata": {
        "id": "6T1MpM9dZE86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify that the Fare Values Add Up"
      ],
      "metadata": {
        "id": "c1Q7azlQkiNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating total amounts and dropping rows whose values don't \"add up\"...\n",
        "fare = td['fare_amount'].values\n",
        "extra = np.add(fare, td['extra'].values)\n",
        "mta_tax = np.add(extra, td['mta_tax'].values)\n",
        "tip_amount = np.add(mta_tax, td['tip_amount'].values)\n",
        "imp_surcharge = np.add(tip_amount, td['imp_surcharge'].values)\n",
        "calculated_total_amount = np.add(imp_surcharge, td['tolls_amount'].values)\n",
        "\n",
        "td['calculated_total_amount'] = pd.Series(calculated_total_amount)\n",
        "\n",
        "# validate calculated total by manually adding all relevant columns and comparing to the calculated column\n",
        "td.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:42.349904Z",
          "iopub.execute_input": "2022-06-03T18:37:42.350779Z",
          "iopub.status.idle": "2022-06-03T18:37:42.832377Z",
          "shell.execute_reply.started": "2022-06-03T18:37:42.350739Z",
          "shell.execute_reply": "2022-06-03T18:37:42.831487Z"
        },
        "trusted": true,
        "id": "-kyWedbFBxoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping incorrect `total_amount` values"
      ],
      "metadata": {
        "id": "Wsmi2rlXQDCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping incorrect `total_amount` values\n",
        "init_count = len(td)\n",
        "td = td[td['total_amount'] != td['calculated_total_amount']]\n",
        "final_count = len(td)\n",
        "\n",
        "print(f\"Fraction of dataframe retained: {final_count / init_count * 100}%\")\n",
        "\n",
        "# Drop the computed fare value\n",
        "td.drop('calculated_total_amount', axis=1, inplace=True)\n",
        "\n",
        "td.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:42.833361Z",
          "iopub.execute_input": "2022-06-03T18:37:42.833554Z",
          "iopub.status.idle": "2022-06-03T18:37:44.680674Z",
          "shell.execute_reply.started": "2022-06-03T18:37:42.833529Z",
          "shell.execute_reply": "2022-06-03T18:37:44.679868Z"
        },
        "trusted": true,
        "id": "etjvAc1pBxoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataframe After Cleanup"
      ],
      "metadata": {
        "id": "ZX0Xv0W1bSn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample rows from the dataset\n",
        "td.head()"
      ],
      "metadata": {
        "id": "p_SM_LdDbfTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Transformation\n",
        "\n",
        "The following data transformation steps are applied to the cleansed data, to facilitate further data analysis and mining."
      ],
      "metadata": {
        "id": "GczZNNuInNqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct Composite/Derived Attributes\n",
        "\n",
        "Construction of composite and derived attributes can greatly aid the the learning and analysis phase of data mining. Simplex relationships across the data attributes that may not be captured by the downstream analysis models, can be expressed through explicity computed attributes through a combination of one or more pre-existing attributes."
      ],
      "metadata": {
        "id": "A27GdyFARYnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute and Add Driving Speed\n",
        "\n",
        "A `driving_speed` attribute in addition to the existing data attributes can be useful to analyze traffic data in different geographic locations of the city. Average speed can be directly correlated with the traffic density."
      ],
      "metadata": {
        "id": "hR7edI8rncc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trip_distance_array = td['trip_distance']\n",
        "trip_duration_array = td['trip_duration']\n",
        "\n",
        "# driving_speed = trip_dist (in miles) / trip_duration (in sec) * 3600 sec\n",
        "driving_speed = np.divide(trip_distance_array, trip_duration_array)*3600\n",
        "\n",
        "td['driving_speed'] = pd.Series(trip_duration)"
      ],
      "metadata": {
        "id": "qb2LKg2Un6QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute and Add Tipping Rate\n",
        "\n",
        "`tipping_rate` can help to analyze the general proportion of tipping that cab riders usually pay for their rides."
      ],
      "metadata": {
        "id": "ixWY62yHo9aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tip_amount_array = td['tip_amount']\n",
        "total_amount_array = td['total_amount']\n",
        "\n",
        "# tipping rate = tip_amount / total_amount\n",
        "tipping_rate = np.divide(tip_amount_array, total_amount_array)\n",
        "\n",
        "td['tipping_rate'] = tipping_rate"
      ],
      "metadata": {
        "id": "6dP5gck-pYzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Normalization\n",
        "\n",
        "Data Normalization is a typical practice in data mining technique which consists of transforming numeric columns to a standard scale. Since some feature values differ from others multiple times, the features with higher values will dominate the learning and analysis process. Hence, bringing them down to the same scale is useful for faster and meaningful analysis.\n",
        "\n",
        "Simple `min-max scaling` procedure is adopted in the following cells to normalize specific attributes. Since statistical information about the mean, variance, etc. is not known, more complex and informed scaling procedures cannot be applied."
      ],
      "metadata": {
        "id": "BCC_atnoRIQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Fare Attributes\n",
        "\n",
        "All cost related columns have very distinct scales. `tip_amount`, for instance, is exteremely low while the `total_fare` is typically a very high value. Hence, the latter would dominate the analysis and model training (for ML) processes. These are normalized here,"
      ],
      "metadata": {
        "id": "oWISbxPFTNpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td_min_max_scaled = td.copy()\n",
        "  \n",
        "columns = [\n",
        "    'fare_amount',\n",
        "    'extra',\n",
        "    'mta_tax',\n",
        "    'tip_amount',\n",
        "    'tolls_amount',\n",
        "    'total_amount'\n",
        "]\n",
        "for column in columns:\n",
        "  td_min_max_scaled[column] = (td_min_max_scaled[column] - td_min_max_scaled[column].min()) / (td_min_max_scaled[column].max() - td_min_max_scaled[column].min())    \n",
        "\n",
        "td_min_max_scaled.head()\n",
        "\n",
        "del td\n",
        "td = td_min_max_scaled"
      ],
      "metadata": {
        "id": "QC7F4eQ6Sw6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numeric Encoding for Categorical String Attributes\n",
        "\n",
        "Encoding categorical data is a process of converting categorical data into integer format so that the data with converted categorical values can be provided to the models to give and improve the predictions."
      ],
      "metadata": {
        "id": "-SNUZT_eYMAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode `store_and_fwd_flag` Attribute"
      ],
      "metadata": {
        "id": "spjto5kfY7Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td_encoded = td.copy()\n",
        "\n",
        "td.store_and_fwd_flag = td.store_and_fwd_flag.astype('category').cat.codes\n",
        "\n",
        "del td\n",
        "td = td_encoded\n",
        "td.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "tnog8-_KZCFW",
        "outputId": "d61122ac-b2df-4d58-aff6-d5360de28825"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-11346dfbc45b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtd_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_and_fwd_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_and_fwd_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'td' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finishing Up  \n",
        "\n",
        "The *total_amount* column did a lot more than just clean totals, but it actually checked all of the other total effecting columns at the same time. If any errors occurred in any column, the calculated total would have differed from the calculated total. \n",
        "\n",
        "Missing *mta_tax*, and incorrect *toll_amount* values are dropped. "
      ],
      "metadata": {
        "id": "U0_Q_EwMBxoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a quick, easy way to de-allocate the memory assigned to df, which holds the original dataframe\n",
        "# this was necessary else the write to csv function of Pandas (to_csv) would max out the allowed memory in the notebook environment on Kaggle.\n",
        "df=[]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:44.681957Z",
          "iopub.execute_input": "2022-06-03T18:37:44.68219Z",
          "iopub.status.idle": "2022-06-03T18:37:44.686822Z",
          "shell.execute_reply.started": "2022-06-03T18:37:44.682162Z",
          "shell.execute_reply": "2022-06-03T18:37:44.685719Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "9L9H0cckBxoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the cleaned dataframe to a CSV."
      ],
      "metadata": {
        "id": "ydDRCuhtQinV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td.to_csv(os.path.join(DATA_PATH, 'taxi-trip-data_2018_cleaned.csv'))\n",
        "print('Done!')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:37:44.688327Z",
          "iopub.execute_input": "2022-06-03T18:37:44.688636Z",
          "iopub.status.idle": "2022-06-03T18:38:55.21389Z",
          "shell.execute_reply.started": "2022-06-03T18:37:44.688607Z",
          "shell.execute_reply": "2022-06-03T18:38:55.212812Z"
        },
        "trusted": true,
        "id": "rCGT6XEiBxoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the notebook"
      ],
      "metadata": {
        "id": "hs6H1N-HqVPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3txfaVCqU0Q",
        "outputId": "b2530325-4a20-4ac7-a235-3d95ab154bec"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "pandoc is already the newest version (1.19.2.4~dfsg-1build4).\n",
            "texlive is already the newest version (2017.20180305-1).\n",
            "texlive-latex-extra is already the newest version (2017.20180305-2).\n",
            "texlive-xetex is already the newest version (2017.20180305-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.7/dist-packages (1.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Undergrad/Semester-7/DWDM_Preprocessing-Assignment\")\n",
        "!ls\n",
        "!jupyter nbconvert --to PDF \"Analysis-Cleaning-Transformation.ipynb\"\n",
        "\n",
        "# Save processed CSV\n",
        "td.to_csv(os.path.join(DATA_PATH, 'taxi-trip-data_2018_cleaned.csv'))\n",
        "print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfmaTRGyqhLE",
        "outputId": "cb17f440-a7f3-407a-ece2-50d71970cd5c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_Analysis.ipynb  Analysis-Cleaning-Transformation.ipynb\n",
            "2_Cleaning.ipynb  Data\n",
            "[NbConvertApp] Converting notebook Analysis-Cleaning-Transformation.ipynb to PDF\n",
            "[NbConvertApp] Support files will be in Analysis-Cleaning-Transformation_files/\n",
            "[NbConvertApp] Making directory ./Analysis-Cleaning-Transformation_files\n",
            "[NbConvertApp] Writing 110780 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 218153 bytes to Analysis-Cleaning-Transformation.pdf\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}